# -*- coding: utf-8 -*-
"""Customer segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9u_EmjK14_-R2h-qMZ4qMgBwL-YUUYl

# Part 1: Data preprocessing

dataset link: https://www.kaggle.com/datasets/arjunbhasin2013/ccdata?select=CC+GENERAL.csv

## Importing the libraries and dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""numpy is to deal with array and pandas is for data set and data frame and matplotlib and seaborn are data visulization lib"""

# read the data set
dataset = pd.read_csv('/content/CC GENERAL.csv')

dataset.head()

"""## Data Exploration"""

dataset.shape

dataset.columns

# Check the info about dataset
dataset.info()

# Checking the categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

# Checking the numerical cloumns
dataset.select_dtypes(include=['int64','float64']).columns

len(dataset.select_dtypes(include=['int64','float64']).columns)

# Checking the statistical summary
dataset.describe()

"""## Dealing with missing values"""

dataset.isnull().values.any()

dataset.isnull().values.sum()

"""There are total 314 null values in this dataset"""

# Checking the columns with the null values
dataset.columns[dataset.isnull().any()]

"""The columns with null values are numerical data type to replace this we are going to take the mean and add"""

dataset['CREDIT_LIMIT'] = dataset['CREDIT_LIMIT'].fillna(dataset['CREDIT_LIMIT'].mean())
dataset['MINIMUM_PAYMENTS'] = dataset['MINIMUM_PAYMENTS'].fillna(dataset['MINIMUM_PAYMENTS'].mean())

len(dataset.columns[dataset.isnull().any()])

"""## Encoding categorical data"""

# Checking the categorical columns
dataset.select_dtypes(include='object').columns

"""These are only customer_id and do make sense to our machine learning model in future and it means we can drop this column"""

dataset.head()

dataset = dataset.drop(columns='CUST_ID')

dataset.head()

# Checking the categorical columns
dataset.select_dtypes(include='object').columns

"""## Correlation matrix"""

# Defining a variable correlation
corr = dataset.corr()

# heatmap
plt.figure(figsize=(16,9))
ax = sns.heatmap(corr,annot=True,cmap='coolwarm')

"""## Splitting the dataset"""

# We only have independent variable so there is nothing to split there is
# only x variable not y target variable

"""## Feature Scaling"""

# defining a variable df which is before scaling the dataset
df = dataset

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
dataset = sc.fit_transform(dataset)

dataset
# After appling feature scaling all the variable will be in the same raange

"""# Part 2: Elbow method (finding the optimal number of clusters)"""

from sklearn.cluster import KMeans

# To define optimal number of cluters
# wcss means within clusters sum of square is a list we have defined

# we are tring 20 cluters and out of these we are finding the optimal number

# Craeted a instance of the class kmeans
# then train the dataset

# inertia_ = sum of squared distances of samples to the closest cluster center
wcss = []
for i in range(1, 20):
    kmeans = KMeans(n_clusters=i, init='k-means++')
    kmeans.fit(dataset)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.plot(range(1, 20), wcss, 'bx-')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""Optimal number of cluster is where there is big change, choosing 8 as optimal num of cluster

# Part 3: Building the model
"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=8, init = 'k-means++', random_state=0)

# dependent variables
y_kmeans = kmeans.fit_predict(dataset)

# Return th dependent variable
y_kmeans

"""# Part 4 : Getting the output"""

y_kmeans.shape

y_kmeans = y_kmeans.reshape(len(y_kmeans),1)

y_kmeans.shape

b = np.concatenate((y_kmeans,df),axis=1) # Concatenating with the original dataset and the dependent variable

df.columns

df_final = pd.DataFrame(data=b, columns=['Cluster_Number','BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',
       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',
       'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',
       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',
       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',
       'TENURE'])

df_final.head()

df_final.to_csv('SegmentedCustomers')

